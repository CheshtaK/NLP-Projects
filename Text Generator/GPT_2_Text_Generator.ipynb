{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Text Generator.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OxJU9eTktQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "06093907-6da2-4f4d-c40c-c5d5f8a81e76"
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 203, done.\u001b[K\n",
            "remote: Total 203 (delta 0), reused 0 (delta 0), pack-reused 203\u001b[K\n",
            "Receiving objects: 100% (203/203), 4.37 MiB | 14.82 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOSKCxOPk6o-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"gpt-2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHmx3YMemcrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "558b2df6-46c4-4132-8252-3c3e06199c4d"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.9MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=fc21cc31820e0c34def8fa5a2c3885dd953b8b77831c79c10d073e5a4bae10a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533178 sha256=d5ac1535cf67687f6c77e36b1a23774fd805ee81854ee686af77a21035824191\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1q-XIqnmsL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0771fb7b-b3b7-4bf2-d549-4bbbe3fdde10"
      },
      "source": [
        "!python3 download_model.py 345M"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 843kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 51.5Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 922kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:21, 64.5Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.95Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 48.7Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 48.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5v4GMkam0nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsGQByuTm7Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('src')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz49nz5Em_3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akU2sryKnBv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact_model(\n",
        "    model_name,\n",
        "    seed,\n",
        "    nsamples,\n",
        "    batch_size,\n",
        "    length,\n",
        "    temperature,\n",
        "    top_k,\n",
        "    models_dir\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV3ASbtOnOjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3c9a1e0-0f13-46ad-cc5c-f0b88e277846"
      },
      "source": [
        "interact_model(\n",
        "    '345M',\n",
        "    None,\n",
        "    1,\n",
        "    1,\n",
        "    300,\n",
        "    1,\n",
        "    0,\n",
        "    '/content/gpt-2/models'\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0822 14:01:15.747022 139686458316672 deprecation_wrapper.py:119] From /content/gpt-2/src/sample.py:33: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0822 14:01:15.748268 139686458316672 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0822 14:01:15.760870 139686458316672 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0822 14:01:15.806949 139686458316672 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "W0822 14:01:21.063520 139686458316672 deprecation.py:323] From /content/gpt-2/src/sample.py:46: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0822 14:01:21.067246 139686458316672 deprecation.py:323] From /content/gpt-2/src/sample.py:48: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0822 14:01:25.950191 139686458316672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model prompt >>> I went to a lounge to celebrate my birthday and \n",
            "======================================== SAMPLE 1 ========================================\n",
            " OP became brought in from Uni, as I figured the guys that were there would know who OP was, in case I forgot or something. Being Protestant in devil land, Smart two West res friends got a bit bored going between the guys, I just wanted to chase it and did. I ended up following another OP (who forgot OP's name) and was like those guys op said OP was looking for, he was ecstatic and wrote.\n",
            " What I love about tumblr is there are many genres we have a bit of fun extremelyif you are into things like my Navy background was cool lol but seriously people benefit is being able to share their interests and this club does in many cases. Anyone here that is into plants, ice cream or family is in, at least 35% of the population of lsu and it really help for people who now live full time in town because their ability to pursue what they are passionate about does confer, if they want to when it happens just try to come out and see that occasional shows going down or like i mention but IM MAKING SOCIAL COMFORT was going to lessen some just thinking about guys who don't like the idea. Put that in your bio and flash people who is participating at a venue you like, Make him well known ; )\n",
            "The Def Pen style is amazing, when someone told you they Love tumblr this is an example of having a post on tumblr to describe yourself doing, anyway you come here a lot looking for\n",
            "================================================================================\n",
            "Model prompt >>> I went to the zoo and there I saw\n",
            "======================================== SAMPLE 1 ========================================\n",
            " the large, black, macaque leopard cubs hanging from the thrones just curious. I pleaded little wino not to disobey my orders and after some hesitation, cuddled up to the cubs and let my eyes roll in their eyes. Sadly, the black leopard cubs grouped up together with the cubs of the other 8 leopards and started barreling back towards us. At first, I tried to swoop in and save them, but they ran away like leopard cubs running away from a dead clicker. It was too late to save them. I then gave chase lowering my knees higher ~less realized just how high my legs were trying to reach and what terrible ugliness was spreading out from where I was drooling through my fur cover. At this point it was just me coming up behind them and slinging my paws and head up towards give them more space. They leapt, hitting each other, as they tried going after each other…which just intensified as each cub ran towards me! Halfway into their run back off the ground to suck my air! And now the cubs of the other 10 leopards were gnashing their talons on my shin and arms…to add insult to injury, Monkey's growing irritability began to spread and I started to get second thoughts. I was about to bring down the teh megabeast with a huge blow but I realized get scared now. At this point I stole a few breaths\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7d832fd16da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m'/content/gpt-2/models'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-8-9bbb37a09480>\u001b[0m in \u001b[0;36minteract_model\u001b[0;34m(model_name, seed, nsamples, batch_size, length, temperature, top_k, models_dir)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model prompt >>> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prompt should not be empty!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrYmYQTvoZch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}